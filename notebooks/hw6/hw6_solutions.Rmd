---
title: "STAT 8830 HW6"
output: html_notebook
---

# 1) Finding best k for k-nearest neighbors with LOOCV

Here we want to define a function to apply the $k$-nearest neighbors algorithm to a labeled dataset and find the optimal value of $k$ with leave-one-out cross-validation: that is the value of $k$ which yields the best estimates, on average, when a single value is masked and predicted using the rest of the training set. Given a set of labeled pairs $(X_i, Y_i)$, $i \in {1, ..., n}$, $k$ can range from $1$ to $n - 1$. We exhaustively test all possibilities and select the one that maximizes correct predictions on the training set.

```{r}
library(plyr)
library(ggplot2)
library(class)

KNNcv = function(train, test, cl) {
  ks = seq(1, nrow(train) - 1)
  props_correct = c()
  
  # loop over all values of k, perform LOOCV, count proportion of correct predictions
  for (k in ks) {
    num_correct = 0
    # loop over rows and validate with each one
    for (i in 1:nrow(train)) {
      real = cl[i]
      pred = knn(train[-i,], train[i,], cl=cl[-i], k=k)
      if (pred == real) num_correct = num_correct + 1
    }
    props_correct = c(props_correct, num_correct / nrow(train))
    print(paste(num_correct, 'correct for k =', k))
  }
  
  # if multiple values of k are tied for optimal, just pick one randomly
  best_k = which.max(props_correct)
  best_prop = props_correct[best_k]
  print(paste0('Best k=', best_k, ' with ', round(best_prop, digits = 2) * 100, '% correct'))
  
  # fit knn to test data and return predictions and best k
  list(knn(train, test, cl, best_k), best_k)
}
```

First, generate a dataset with 3 normally distributed clusters.

```{r}
library(MASS)
library(gmodels)
library(ggplot2)

set.seed(42)

n = 100  # points per cluster
mu_x1 = 10
mu_y1 = 10
mu_x2 = 30
mu_y2 = 10
mu_x3 = 30
mu_y3 = 30
sd = 60
d1 = mvrnorm(n, mu=c(mu_x1, mu_y1), Sigma=sd * diag(2))
d2 = mvrnorm(n, mu=c(mu_x2, mu_y2), Sigma=sd * diag(2))
d3 = mvrnorm(n, mu=c(mu_x3, mu_y3), Sigma=sd * diag(2))
d = rbind(d1, d2, d3)
d = cbind(d, c(rep(1, n), rep(2, n), rep(3, n)))
colnames(d) = c('x', 'y', 'category')
plot(d[,1:2], col=d[,3], xlab="X1", ylab="X2", main="Sample Data", pch=16)
legend("bottomright", c("category 1", "category 2", "category 3"), col=1:3, pch=16)
```

Split the data into subsets for training and testing.

```{r}
train = sample(1:nrow(d), nrow(d) / 2)
d.train = d[train,]
d.test = d[-train,]
```

Now run the simulation function.

```{r}
result = KNNcv(d.train[, c("x", "y")], d.test[, c("x", "y")], d.train[, 'category'])
test_preds = result[[1]]
best_k = result[[2]]

X = seq(min(d.train[, 'x']) - 1, max(d.train[, 'x']) + 1, by = 1)
Y = seq(min(d.train[, 'y']) - 1, max(d.train[, 'y']) + 1, by = 1)
grid = expand.grid(X, Y)
colnames(grid) = c('x', 'y')

plot(d.test[, c("x")], d.test[, c("y")], xlab = 'X', ylab = 'Y', pch = as.numeric(d.test[, 'category']) + 1, main = paste('Predictions for k =', best_k))
legend(x = 'topleft', legend = c("Correct", "Incorrect"),  fill = c("green", "red"))
points(d.test[, c("x", "y")], col = ifelse(test_preds == d.test[, 'category'], 'springgreen3', 'tomato'))
```

# 2) Comparison of LDA, KNN, and tree classification

Here we want to compare the accuracy of LDA, KNN, and tree classification.

First we generate a test dataset with 2 classes. To generate complex boundaries we can use a mixture distribution such that the second class is normal with 1/2 odds of mean (30, 10) and sd 20 and 1/2 of mean(20, 40) and sd 60.

```{r}
n = 100  # points per cluster
mu_x1 = 10
mu_y1 = 10
sd1 = 40
mu_x2 = 30
mu_y2 = 10
sd2 = 20
mu_x3 = 20
mu_y3 = 40
sd3 = 60
d1 = mvrnorm(n, mu=c(mu_x1, mu_y1), Sigma=sd1 * diag(2))
d2 = mvrnorm(n, mu=c(mu_x2, mu_y2), Sigma=sd2 * diag(2))
d3 = mvrnorm(n, mu=c(mu_x3, mu_y3), Sigma=sd3 * diag(2))
d = rbind(d1, d2, d3)
d = cbind(d, c(rep(1, n), rep(2, 2 * n)))
colnames(d) = c('x', 'y', 'category')
plot(d[,1:2], col=d[,3], xlab="X", ylab="Y", main="Sample Data", pch=16)
legend("bottomright", c("category 1", "category 2"), col=1:2, pch=16)
```

