---
title: "STAT 8830 HW6"
output: html_notebook
---

# 1) Finding best k for k-nearest neighbors with LOOCV

Here we will define a function to apply the $k$-nearest neighbors algorithm to a given labeled dataset and find the optimal value of $k$ with leave-one-out cross-validation: that is the value of $k$ which yields the best estimates, on average, when a value is masked and predicted from its neighborhood. Given a set of labeled pairs $(X_i, Y_i)$, $i \in {1, ..., n}$, $k$ can range from $1$ to $n - 1$. We exhaustively test all possibilities and select the one that minimizes mean squared error.

```{r}
library(plyr)
library(ggplot2)
library(class)

KNNcv = function(train, test, cl) {
  ks = seq(1, min(nrow(train), nrow(test)) - 1)
  minerror = length(ks)
  mink = 1
  minpreds = NULL
  
  for (k in ks) {
    fit = knn.cv(train, cl, k=k, prob=T)
    table(fit)
}
```

First, generate a dataset with randomly located, normally distributed clusters.

```{r}
library(MASS)
library(ggplot2)

set.seed(42)

n = 10  # points per cluster
mu_x1 = 10
mu_y1 = 10
mu_x2 = 30
mu_y2 = 10
mu_x3 = 30
mu_y3 = 30
sd = 60
d1 = mvrnorm(n, mu=c(mu_x1, mu_y1), Sigma=sd * diag(2))
d2 = mvrnorm(n, mu=c(mu_x2, mu_y2), Sigma=sd * diag(2))
d3 = mvrnorm(n, mu=c(mu_x3, mu_y3), Sigma=sd * diag(2))
d = rbind(d1, d2, d3)
d = cbind(d, c(rep(1, n), rep(2, n), rep(3, n)))
colnames(d) = c('x', 'y', 'category')
plot(d[,1:2], col=d[,3], xlab="X1", ylab="X2", main="Sample Data", pch=16)
legend("bottomright", c("category 1", "category 2", "category 3"), col=1:3, pch=16)
```

Now split the data into subsets for training and testing.

```{r}
train = sample(1:nrow(d), nrow(d) / 2)
d.train = d[train,]
d.test = d[-train,]
```

Now run the simulation function.

```{r}
result = KNNcv(d.train[, c("x", "y")], d.test[, c("x", "y")], d.train[, c('category')])
k = result[[1]]
preds = result[[2]]
print(k)
print(preds)
```

